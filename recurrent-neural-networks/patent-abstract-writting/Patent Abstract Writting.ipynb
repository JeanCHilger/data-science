{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Program to Write Patent Abstracts\n",
    "\n",
    "**Basic problem setting:** Give the network a sequence of words and train it to predict the next word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/jean/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jean/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jean/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jean/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jean/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jean/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/jean/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jean/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jean/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jean/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jean/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jean/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# Importing libraries\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv and aquire abstracts as a list of strings\n",
    "abstracts = []\n",
    "\n",
    "with open(\"./data/b9939483.csv\") as abs_file:\n",
    "    reader = csv.reader(abs_file)\n",
    "    for row in reader:\n",
    "        abstracts.append(row[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out that `abstracts` is a list of strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The assignment of phonemes to graphemes producing them in a lexicon having words (grapheme sequences)\n"
     ]
    }
   ],
   "source": [
    "# First 101 characters of the 100th abstract. It show us the first 15 words in the abstract.\n",
    "print(abstracts[100][:101])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to turn the words in a format meaningful to a neural network. For that we'll use keras' `Tokenizer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a tokenizer\n",
    "\n",
    "# Note we're are removing neither ponctuations nor uppercased letters. \n",
    "# If we removed, our network wouldn't be learning proper english .\n",
    "tokenizer = Tokenizer(num_words=None,\n",
    "                      filters='#$%&()*+-<=>@[\\\\]^_`{|}~\\t\\n',\n",
    "                      lower=False,\n",
    "                      split=' ')\n",
    "\n",
    "# Updates tokenizer internal word dictionary\n",
    "tokenizer.fit_on_texts(abstracts)\n",
    "\n",
    "# Turn texts into sequences\n",
    "abs_sequences = tokenizer.texts_to_sequences(abstracts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can see that our data is no longer text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 3411, 2, 786, 5, 4101, 739, 1102, 9, 3, 7181, 78, 272, 1629, 651]\n"
     ]
    }
   ],
   "source": [
    "# `abs_sequences` is now a list of integers. Every integer denotes a word. Compare with previous result\n",
    "print(abs_sequences[100][:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These number are bsically indexes of the inner dictionary of `Tokenizer`. We can see what every number means by doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The assignment of phonemes to graphemes producing them in a lexicon having words grapheme sequences'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_idx = tokenizer.index_word\n",
    "\" \".join(words_idx[w] for w in abs_sequences[100][:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note we've got the same first 15 words as by getting the first 101 characters of `abstracts[100]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Features and Labels\n",
    "\n",
    "To train our RNN we'll use the following strategy: we'll feed the network with 50 words and it will have to predict the 51st word. With that said, define the features and the labels is pretty simple.\n",
    "\n",
    "First, the words 1-50 will be used as features and the 51st will be the label. Next the features will be words 2-51 and 52nd will be the label, and so on. This process will be repeated for every abstract, till its end. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  11   18    4    3   11 1075    4   78    3   39    2  327  246  603\n",
      "    9  148   64   12   18   53    4   12   29  525  105  118  133    3\n",
      " 2852    2  118  155    4    3   42    2  319    7  257  294    1  118\n",
      "  155    5  124    3  118  526   20   39]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "X = [] # Features\n",
    "y = [] # Labels\n",
    "\n",
    "training_length = 50\n",
    "\n",
    "# Iterate over all abstracts\n",
    "for seq in abs_sequences:\n",
    "\n",
    "    # For every abstract, create several examples\n",
    "    for i in range(0, len(seq) - training_length):\n",
    "        \n",
    "        # X will receive the words in the interval [i, i + 50[\n",
    "        X.append(seq[i:i+training_length])\n",
    "        \n",
    "        # y will receive the (i + 50)-th word \n",
    "        y.append(seq[i+training_length])\n",
    "\n",
    "X = np.array(X)\n",
    "\n",
    "# Check 100th feature and its label (not the same as the 100th entry we've been using as example)\n",
    "print(X[100])\n",
    "print(y[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how many examples we end up with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102883, 50)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That means we have 102883 sequences each with 50 tokens. In the neural networks terminology, every sequence has 50 timesteps with 1 feature each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The leabels could be kept as integers, but the neural net will learn better if we one-hot encode the labels. With numpy we do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  11   18    4    3   11 1075    4   78    3   39    2  327  246  603\n",
      "    9  148   64   12   18   53    4   12   29  525  105  118  133    3\n",
      " 2852    2  118  155    4    3   42    2  319    7  257  294    1  118\n",
      "  155    5  124    3  118  526   20   39]\n",
      "[0 0 1 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Get number of words in vocabulary\n",
    "num_words = len(words_idx) + 1\n",
    "\n",
    "# Create array to hold one-hot encoded labels\n",
    "y_one_hot = np.zeros((len(X), num_words), dtype=np.int8)\n",
    "\n",
    "# Encode labels\n",
    "for x_index, word_index in enumerate(y):\n",
    "    y_one_hot[x_index, word_index] = 1\n",
    "    \n",
    "# Check 100th feature and its one-hot encoded label (compare with previous label)\n",
    "print(X[100])\n",
    "print(y_one_hot[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to see what an entry in the array means we can do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'of'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_idx[np.argmax(y_one_hot[100])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our final step with features and labels is to split data into training and testing sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting with the Neural Network\n",
    "\n",
    "Now we'll build the RNN using LSTM (Long Short-Term Memory) cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Masking, Embedding\n",
    "\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using the keras' `Sequential` model, which means the netowork will be build as a linear stack of layers.\n",
    "Next, we define the layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jean/.local/lib/python3.6/site-packages/tensorflow/python/keras/backend.py:3794: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# Embedding layer\n",
    "model.add(\n",
    "    Embedding(input_dim=num_words,\n",
    "              input_length = training_length,\n",
    "              output_dim=100,\n",
    "              weights=None,\n",
    "              trainable=False,\n",
    "              mask_zero=True))\n",
    "\n",
    "# Masking layer for pre-trained embeddings\n",
    "model.add(Masking(mask_value=0.0))\n",
    "\n",
    "# Recurrent layer\n",
    "model.add(LSTM(64, return_sequences=False, \n",
    "               dropout=0.1, recurrent_dropout=0.1))\n",
    "\n",
    "# Fully connected layer\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "\n",
    "# Dropout for regularization\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(num_words, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** For more information about the different layers used, consult keras [documentation](https://keras.io/layers/about-keras-layers/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to compile the model, to set it read for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\",\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Add Pre-trained Embbedings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "Now we can train the model created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Callbacks\n",
    "\n",
    "Before actually training, we have a thing to set. Is a good practice to use `ModelCheckpoint` and `EarlyStopping` as keras callbacks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
