{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Program to Write Patent Abstracts\n",
    "\n",
    "**Basic problem setting:** Give the network a sequence of words and train it to predict the next word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv and aquire abstracts as a list of strings\n",
    "abstracts = []\n",
    "\n",
    "with open(\"./data/b9939483.csv\") as abs_file:\n",
    "    reader = csv.reader(abs_file)\n",
    "    for row in reader:\n",
    "        abstracts.append(row[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out that `abstracts` is a list of strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The assignment of phonemes to graphemes producing them in a lexicon having words (grapheme sequences)\n"
     ]
    }
   ],
   "source": [
    "# First 101 characters of the 100th abstract. It show us the first 15 words in the abstract.\n",
    "print(abstracts[100][:101])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to turn the words in a format meaningful to a neural network. For that we'll use keras' `Tokenizer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a tokenizer\n",
    "\n",
    "# Note we're are removing neither ponctuations nor uppercased letters. \n",
    "# If we removed, our network wouldn't be learning proper english .\n",
    "tokenizer = Tokenizer(num_words=None,\n",
    "                      filters='#$%&()*+-<=>@[\\\\]^_`{|}~\\t\\n',\n",
    "                      lower=False,\n",
    "                      split=' ')\n",
    "\n",
    "# Updates tokenizer internal word dictionary\n",
    "tokenizer.fit_on_texts(abstracts)\n",
    "\n",
    "# Turn texts into sequences\n",
    "abs_sequences = tokenizer.texts_to_sequences(abstracts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can see that our data is no longer text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 3411, 2, 786, 5, 4101, 739, 1102, 9, 3, 7181, 78, 272, 1629, 651]\n"
     ]
    }
   ],
   "source": [
    "# `abs_sequences` is now a list of integers. Every integer denotes a word. Compare with previous result\n",
    "print(abs_sequences[100][:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These number are bsically indexes of the inner dictionary of `Tokenizer`. We can see what every number means by doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The assignment of phonemes to graphemes producing them in a lexicon having words grapheme sequences'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_idx = tokenizer.index_word\n",
    "\" \".join(words_idx[w] for w in abs_sequences[100][:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note we've got the same first 15 words as by getting the first 101 characters of `abstracts[100]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Features and Labels\n",
    "\n",
    "To train our RNN we'll use the following strategy: we'll feed the network with 50 words and it will have to predict the 51st word. With that said, define the features and the labels is pretty simple.\n",
    "\n",
    "First, the words 1-50 will be used as features and the 51st will be the label. Next the features will be words 2-51 and 52nd will be the label, and so on. This process will be repeated for every abstract, till its end. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  11   18    4    3   11 1075    4   78    3   39    2  327  246  603\n",
      "    9  148   64   12   18   53    4   12   29  525  105  118  133    3\n",
      " 2852    2  118  155    4    3   42    2  319    7  257  294    1  118\n",
      "  155    5  124    3  118  526   20   39]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "X = [] # Features\n",
    "y = [] # Labels\n",
    "\n",
    "training_length = 50\n",
    "\n",
    "# Iterate over all abstracts\n",
    "for seq in abs_sequences:\n",
    "\n",
    "    # For every abstract, create several examples\n",
    "    for i in range(0, len(seq) - training_length):\n",
    "        \n",
    "        # X will receive the words in the interval [i, i + 50[\n",
    "        X.append(seq[i:i+training_length])\n",
    "        \n",
    "        # y will receive the (i + 50)-th word \n",
    "        y.append(seq[i+training_length])\n",
    "\n",
    "X = np.array(X)\n",
    "\n",
    "# Check 100th feature and its label (not the same as the 100th entry we've been using as example)\n",
    "print(X[100])\n",
    "print(y[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how many examples we end up with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102883, 50)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That means we have 102883 sequences each with 50 tokens. In the neural networks terminology, every sequence has 50 timesteps with 1 feature each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The leabels could be kept as integers, but the neural net will learn better if we one-hot encode the labels. With numpy we do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  11   18    4    3   11 1075    4   78    3   39    2  327  246  603\n",
      "    9  148   64   12   18   53    4   12   29  525  105  118  133    3\n",
      " 2852    2  118  155    4    3   42    2  319    7  257  294    1  118\n",
      "  155    5  124    3  118  526   20   39]\n",
      "[0 0 1 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Get number of words in vocabulary\n",
    "num_words = len(words_idx) + 1\n",
    "\n",
    "# Create array to hold one-hot encoded labels\n",
    "y_one_hot = np.zeros((len(X), num_words), dtype=np.int8)\n",
    "\n",
    "# Encode labels\n",
    "for x_index, word_index in enumerate(y):\n",
    "    y_one_hot[x_index, word_index] = 1\n",
    "    \n",
    "# Check 100th feature and its one-hot encoded label (compare with previous label)\n",
    "print(X[100])\n",
    "print(y_one_hot[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to see what an entry in the array means we can do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'of'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_idx[np.argmax(y_one_hot[100])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our final step with features and labels is to split data into training and testing sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting with the Neural Network\n",
    "\n",
    "Now we'll build the RNN using LSTM (Long Short-Term Memory) cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11217 50\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Masking, Embedding\n",
    "\n",
    "model = Sequential()\n",
    "print(num_words, training_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using the keras' `Sequential` model, which means the netowork will be build as a linear stack of layers.\n",
    "Next, we define the layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_18 (Embedding)     (None, None, 100)         1121700   \n",
      "_________________________________________________________________\n",
      "masking_7 (Masking)          (None, None, 100)         0         \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 64)                42240     \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 11217)             729105    \n",
      "=================================================================\n",
      "Total params: 1,897,205\n",
      "Trainable params: 1,897,205\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Embedding layer\n",
    "model.add(\n",
    "    Embedding(input_dim=num_words,\n",
    "              #input_length=training_length,\n",
    "              output_dim=100\n",
    "              #mask_zero=True)\n",
    "    ))\n",
    "\n",
    "# Masking layer for pre-trained embeddings\n",
    "model.add(Masking(mask_value=0.0))\n",
    "\n",
    "# Recurrent layer\n",
    "model.add(LSTM(64, return_sequences=False, \n",
    "               dropout=0.1, recurrent_dropout=0.1))\n",
    "\n",
    "# Fully connected layer\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "\n",
    "# Dropout for regularization\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(num_words, activation=\"softmax\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** For more information about the different layers used, consult keras [documentation](https://keras.io/layers/about-keras-layers/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to compile the model, to set it read for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\",\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Add Pre-trained Embbedings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "Now we can train the model created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Callbacks\n",
    "\n",
    "Before actually training, we have a thing to set. Is a good practice to use `ModelCheckpoint` and `EarlyStopping` as keras callbacks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor=\"val_loss\", patience=5),\n",
    "    ModelCheckpoint(\"./models/model.h5\", save_best_only=\"True\", save_weights_only=\"True\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can finally train our RNN model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_18 to have shape (11217,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-124-eceb180ebb21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m )\n",
      "\u001b[0;32m/home/jean/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1152\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1154\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jean/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    619\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 621\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jean/.local/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    143\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    146\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_18 to have shape (11217,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=2048,\n",
    "    epochs=150,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=(X_val, y_val)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
