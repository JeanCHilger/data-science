{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents:\n",
    "{TODO}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>1 INTRODUCTION</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>2 LINEAR ALGEBRA</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>2.1 SCALARS, VECTORS, MATRICES AND TENSORS</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Scalar**: A single number.\n",
    "* **Vector**: Array of numbers. Each element is identified by an index.\n",
    "* **Matrix**: A 2-D array of numbers. Each element is identified by 2 indexes.\n",
    "* **Tensor**: A n-D array of numbers. Each element is identified by n indexes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>2.2 MULTIPLYING MATRICES AND VECTORS</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>2.3 IDENTITY AND INVERSE MATRICES</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>2.4 LINEAR DEPENDENCE AND SPAN</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>2.5 NORMS</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $L_p$ is defined by: $$\\|\\boldsymbol{x}\\|_p = \\left( \\sum_{i}|x_i|^p \\right)^\\frac{1}{p},$$ and mesures the distance from the origin to the point $\\boldsymbol{x}$.\n",
    "\n",
    "When $p=2$, it is called **Euclidean norm**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>2.6 SPECIAL KINDS OF MATRICES AND VECTORS</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"intro\">2.7 EIGENDECOMPOSITION</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decompose a matrix $\\boldsymbol{A}$ in **eigenvectors** and **eigenvalues**.\n",
    "\n",
    "* **Eigenvector**: vector $\\boldsymbol{v}$ whose multiplication by $\\boldsymbol{A}$ will only scale $\\boldsymbol{v}$: $$\\boldsymbol{Av} = \\lambda\\boldsymbol{v}$$\n",
    "* **Eigenvalue**: scalar $\\lambda$ that grants properties mentioned above to the corresponding eigenvector.\n",
    "\n",
    "$n$ linearly independent eigenvectors - $\\{v^{(1)}, ..., v^{(n)}\\}$ with eigenvalues $\\{\\lambda_1, ..., \\lambda_n\\}$ - can be concatenated in a matrix $\\boldsymbol{V} = [v^{(1)}, ..., v^{(n)}]$. Similarly, the eigenvalues can be concatenated in a vector $\\boldsymbol{\\lambda} = [\\lambda_1, ..., \\lambda_n]$. Thus, the eigendecomposition of $\\boldsymbol{A}$ is given by: $$\\boldsymbol{A}=\\boldsymbol{V}diag(\\boldsymbol{\\lambda})\\boldsymbol{V^{-1}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>2.8 SINGULAR VALUE DECOMPOSITION (SVD)</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decompose a matrix $\\boldsymbol{A}$ in **singular vectors** and **singular values**. The matrix will be written in the form $$\\boldsymbol{A} = \\boldsymbol{U}\\boldsymbol{D}\\boldsymbol{V}^T,$$ where:\n",
    "\n",
    "* $\\boldsymbol{U}$ is a orthogonal matrix whose columns are known as the **left-singular vectors**.\n",
    "* $\\boldsymbol{D}$ is a diagonal matrix. The elements along its diagonal are known as **singular values**.\n",
    "* $\\boldsymbol{V}$ is a orthogonal matrix whose columns are known as the **right-singular vectors**.\n",
    "\n",
    "(If $A_{m \\times n}$, then: $U_{m \\times m}$, $D_{m \\times n}$ and $V_{n \\times n}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>2.9 THE MOORE-PENROSE PSEUDOINVERSE</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pseudoinverse of $\\boldsymbol{A}$ is defined as a matrix $$\\boldsymbol{A}^+ = \\lim_{\\alpha \\to 0}\\left(\\boldsymbol{A}^T \\boldsymbol{A} + \\alpha \\boldsymbol{I} \\right)^{-1} \\boldsymbol{A}^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.10 THE TRACE OPERATOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.11 THE DETERMINANT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The determinant is equal to the product of all eigenvalues of the matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 PROBABILITY AND INFORMATION THEORY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 WHY PROBABILITY?\n",
    "\n",
    "Machine learning deals with many uncertain variables...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 RANDOM VARIABLES\n",
    "\n",
    "A variable that may accept different values randomly. One of these values is called a **state**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 PROBABILITY DISTRIBUTIONS\n",
    "\n",
    "A **probability distribution** describes how a random variable is likely to assume each of its states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 Discrete Variables and Probability Mass Functions\n",
    "\n",
    "A **Probability Mass Function** (PMF) is a function that maps the state of a discrete random variable to the probability of that state occurs.\n",
    "\n",
    "The probability of $x = x'$ is denoted by $P(x')$ (also may be $P(x=x')$).\n",
    "\n",
    "If $x=x'$ is impossible, then $P(x') = 0$. Likewise, if $x=x'$ is certain, then $P(x')=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Continuous Variables and Probability Density Functions\n",
    "\n",
    "A **Probability Density Function** (PDF) does not give a directly mapping between states and its probabilities..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Marginal Probability\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.8 Expectation, Variance and Covariance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Machine Learning Basics\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Learning Algorithms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Capacity, Overfitting and Underfitting\n",
    "\n",
    "* **Generalization**: Ability to perform well on previously unseen data.\n",
    "\n",
    "The factors dertermining how well a machine learning algorithm performs are its ability to:\n",
    "\n",
    "1. Making the training error small.\n",
    "2. Making the gap between training and test error small.\n",
    "\n",
    "The first item, concerns to the problem of **underfitting**, that occurs when the error on training data is too large. The second has to do with the problem of **overfitting** which occurs when the error on training data is small, but the error on test data is large.\n",
    "\n",
    "Both overfitting and underfitting can be controlled by the model's **capacity**. Simply put, a model capacity tells how well a model can fit to a wide variety of functions. The capacity of a model can be controlled by choosing its **hypothesis space**: the set of functions that the algorithm is allowed to select as solution.\n",
    "\n",
    "### 5.2.1 The No Free Lunch Theorem\n",
    "\n",
    "The problems states that all machine learning algorithms has the same error for previously unobserved data, considering the average of all possible data-generating distributions. \n",
    "\n",
    "### 5.2.2 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Hyperparameters and Validation Sets\n",
    "\n",
    "A **hyperparameter** is a setting that can be used to control the algorithm's behaviour. \n",
    "\n",
    "The **validation set** is a set sampled from the train set used to \"train\" (choose better) hyperparameters.\n",
    "\n",
    "### 5.3.1 Cross-Validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Estimators, Bias and Variance\n",
    "\n",
    "### 5.4.1 Point  Estimation\n",
    "\n",
    "Point estimation is the attempt to provide the single best prediction for a quantity of interest (e.g. a parameter).\n",
    "\n",
    "### 5.4.2 Bias\n",
    "\n",
    "{Not understood}\n",
    "\n",
    "### 5.4.3 Variance and Standard Error\n",
    "\n",
    "### 5.4.4 Trading Off Bias and Variance to Minimize Mean Squared Error\n",
    "\n",
    "### 5.4.5 Consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Maximum Likelihood Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 Bayesian Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7 Supervised Learning Algorithms\n",
    "\n",
    "### 5.7.1 Probabilistic Supervised Learning\n",
    "\n",
    "### 5.7.2 Support Vector Machines\n",
    "\n",
    "### 5.7.3 Other Simple Supervised Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.8 Unsupervised Learning Algorithms\n",
    "\n",
    "A classic task for unsupervised learning is find the \"best\" **representation** for the data. Three main criteria for representation are:\n",
    "\n",
    "* Low-dimensional representation;\n",
    "* Sparse representation;\n",
    "* Independent representation;\n",
    "\n",
    "### 5.8.1 Principal Component Analysis\n",
    "\n",
    "### 5.8.2 k-means Clustering\n",
    "\n",
    "* Sort of one-hot enconding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.9 Stochastic Gradient Descent\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.10 Building a Machine Learning Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.11 Challenges Motivating Deep Learning\n",
    "\n",
    "### 5.11.1 The Curse of Dimensionality\n",
    "\n",
    "The **curse of dimesionality** is the phenomenon that makes machine learning tasks extremely more difficult when the number of dimensions on the data increases.\n",
    "\n",
    "### 5.11.2 Local Constancy and Smoothness Regularization\n",
    "\n",
    "**Smoothness prior** or **local constancy prior** states that the function been learned should not change too much within a small region. This means that if we have a answer for $x$ that answer is probaly good for the neighborhood of $x$.\n",
    "\n",
    "### 5.11.3 Manifold Learning\n",
    "\n",
    "A **manifold** is a connected area. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Deep Feedforward Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Example: Learning XOR\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
