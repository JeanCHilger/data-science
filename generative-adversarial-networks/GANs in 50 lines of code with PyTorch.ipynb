{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN example with PyTorch [NOT WORKING]\n",
    "\n",
    "This is intended to be a concise example of Generative Adversarial Networks, therefore no deep explanation is given.\n",
    "\n",
    "In GAN architecture, there are two Neural Networks competing against each other. The first is called generator (we'll give it the $G$ label) and it simply generate new data. The second is a discriminative network (we'll label it as $D$) that is a common classificator.\n",
    "\n",
    "The generator $G$ goal is to fool the discriminator $D$, trying to make it think the generated data are real. The discriminator will try to classify incoming data as real or fake. This is how the $G$ and $D$ compete. \n",
    "\n",
    "Code taken from [this article](https://medium.com/@devnag/generative-adversarial-networks-gans-in-50-lines-of-code-pytorch-e81b79659e3f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.** First we create the function to generate our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distribution_sampler(mu, sigma):\n",
    "    return lambda n: torch.Tensor(np.random.normal(mu, sigma, (1, n)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.** Next we prepare function that creates the input that goes to the generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_generator_input_sampler():\n",
    "    return lambda m, n: torch.rand(m, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.** Next, we create our generator (which is a standard FeedFoward network)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, `nn.Linear` is a linear layer from PyTorch that applies a linear transformation in the data: $y = xA^T + b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    Defines the generator network, G.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, f):\n",
    "        super(Generator, self).__init__()\n",
    "        self.map1 = nn.Linear(input_size, hidden_size)\n",
    "        self.map2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.map3 = nn.Linear(hidden_size, output_size)\n",
    "        self.f = f\n",
    "        \n",
    "    def foward(self, x):\n",
    "        x = self.f(self.map1(x))\n",
    "        x = self.f(self.map2(x))\n",
    "        return self.f(self.map3(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.** Now we create our discriminator (also, a FeedFoward network). Note that the code is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    Defines the discriminator network, D.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, f):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.map1 = nn.Linear(input_size, hidden_size)\n",
    "        self.map2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.map3 = nn.Linear(hidden_size, output_size)\n",
    "        self.f = f\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.f(self.map1(x))\n",
    "        x = self.f(self.map2(x))\n",
    "        return self.f(self.map3(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.** Finally, we create the training loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start the loop, we need to set some things up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_moments(d):\n",
    "    \"\"\"\n",
    "    Return the first 4 moments of the data provided\n",
    "    For more information about moments check the link:\n",
    "    https://www.thoughtco.com/what-are-moments-in-statistics-3126234\n",
    "    \"\"\"\n",
    "    \n",
    "    mean = torch.mean(d)\n",
    "    diffs = d - mean\n",
    "    var = torch.mean(torch.pow(diffs, 2.0))\n",
    "    std = torch.pow(var, 0.5)\n",
    "    zscores = diffs / std\n",
    "    skews = torch.mean(torch.pow(zscores, 3.0))\n",
    "    kurtoses = torch.mean(torch.pow(zscores, 4.0)) - 3.0  # excess kurtosis, should be 0 for Gaussian\n",
    "    final = torch.cat((mean.reshape(1,), std.reshape(1,), skews.reshape(1,), kurtoses.reshape(1,)))\n",
    "    return final\n",
    "\n",
    "# Data params\n",
    "data_mean = 4\n",
    "data_stdev = 1.25\n",
    "\n",
    "g_input_size = 1                # Random noise dimension coming into generator, per output vector\n",
    "g_hidden_size = 5               # Generator hidden layers size\n",
    "g_output_size = 1               # Size of generated output vector\n",
    "\n",
    "d_input_size = 4                # Input size coming into discriminator\n",
    "d_hidden_size = 10              # Discriminator hidden layers size\n",
    "d_output_size = 1               # Single dimension for 'real' vs. 'fake' classification\n",
    "\n",
    "epochs = 5000                   # Training epochs\n",
    "d_steps = 20                    # Train steps for D\n",
    "g_steps = 20                    # Train steps for G\n",
    "\n",
    "minibatch_size = d_input_size   #\n",
    "\n",
    "# Get samplers\n",
    "g_sampler = get_generator_input_sampler()\n",
    "d_sampler = get_distribution_sampler(data_mean, data_stdev)\n",
    "\n",
    "# Instantiates generator\n",
    "G = Generator(input_size=g_input_size,\n",
    "                  hidden_size=g_hidden_size,\n",
    "                  output_size=g_output_size,\n",
    "                  f=torch.tanh)\n",
    "\n",
    "# Instantiates discriminator\n",
    "D = Discriminator(input_size=d_input_size,\n",
    "                  hidden_size=d_hidden_size,\n",
    "                  output_size=d_output_size,\n",
    "                  f=torch.sigmoid)\n",
    "\n",
    "d_learning_rate = 1e-3          # D learning rate\n",
    "g_learning_rate = 1e-3          # G learning rate\n",
    "sgd_momentum = 0.9              #\n",
    "    \n",
    "# Define optimizers for D and G\n",
    "d_optimizer = optim.SGD(D.parameters(), lr=d_learning_rate, momentum=sgd_momentum)\n",
    "g_optimizer = optim.SGD(G.parameters(), lr=g_learning_rate, momentum=sgd_momentum)\n",
    "\n",
    "# Define the cost function as the\n",
    "# binary cross-entropy function.\n",
    "cost = nn.BCELoss()\n",
    "\n",
    "dfe, dre, ge = 0, 0, 0\n",
    "d_real_data, d_fake_data, g_fake_data = None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we perform the actually training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jean/.local/lib/python3.6/site-packages/torch/nn/modules/loss.py:498: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-39051f429b2c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# Train D on fake data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0md_gen_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_sampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminibatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_input_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0md_fake_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_gen_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# detach to avoid G training with these data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0md_fake_decision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_moments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_fake_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0md_fake_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_fake_decision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jean/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jean/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *input)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mregistered\u001b[0m \u001b[0mhooks\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlatter\u001b[0m \u001b[0msilently\u001b[0m \u001b[0mignores\u001b[0m \u001b[0mthem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \"\"\"\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    \n",
    "    # Discriminator loop\n",
    "    for d_index in range(d_steps):\n",
    "        D.zero_grad()\n",
    "        \n",
    "        # Train D on real data\n",
    "        d_real_data = Variable(d_sampler(d_input_size))\n",
    "        d_real_decision = D(get_moments(d_real_data))\n",
    "        d_real_error = cost(d_real_decision, Variable(torch.ones([1, 1])))\n",
    "        d_real_error.backward()\n",
    "        \n",
    "        # Train D on fake data\n",
    "        d_gen_input = Variable(g_sampler(minibatch_size, g_input_size))\n",
    "        d_fake_data = G(d_gen_input).detach()  # detach to avoid G training with these data\n",
    "        d_fake_decision = D(get_moments(d_fake_data.t()))\n",
    "        d_fake_error = cost(d_fake_decision, Variable(torch.ones([1, 1])))\n",
    "        d_fake_error.backward()\n",
    "        d_optimizer.step() # chages D's parameters based on backpropagation results\n",
    "        dre = d_real_error.data.storage().tolist()[0]\n",
    "        dfe = d_fake_error.data.storage().tolist()[0]\n",
    "        \n",
    "    # Generator loop\n",
    "    for g_index in range(g_steps):\n",
    "        G.zero_grad()\n",
    "        \n",
    "        gen_input = Variable(g_sampler(minibatch_size, g_input_size))\n",
    "        g_fake_data = G(gen_input)\n",
    "        dg_fake_decision = D(get_moments(g_fake_data.t()))\n",
    "        g_error = cost(dg_fake_decision, Variable(torch.ones([1, 1])))\n",
    "        \n",
    "        g.backward()\n",
    "        g_optimizer.step()\n",
    "        ge = g_error.data.storage().tolist()[0]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
