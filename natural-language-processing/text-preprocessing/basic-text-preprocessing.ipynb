{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jean/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.1.0.json: 122kB [00:00, 42.0MB/s]                    \n",
      "2020-11-22 21:10:23 INFO: Downloading these customized packages for language: en (English)...\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | ewt     |\n",
      "| lemma     | ewt     |\n",
      "=======================\n",
      "\n",
      "2020-11-22 21:10:23 INFO: File exists: /home/jean/stanza_resources/en/tokenize/ewt.pt.\n",
      "2020-11-22 21:10:23 INFO: File exists: /home/jean/stanza_resources/en/lemma/ewt.pt.\n",
      "2020-11-22 21:10:23 INFO: Finished downloading models and saved to /home/jean/stanza_resources.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "\n",
    "import stanza\n",
    "stanza.download('en', processors='tokenize,lemma')\n",
    "\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['title', 'description', 'text']\n"
     ]
    }
   ],
   "source": [
    "dataset_file_path = \"../../datasets/cbc-news-coronavirus/data.csv\"\n",
    "\n",
    "columns = [\"title\", \"description\", \"text\"]\n",
    "\n",
    "n_docs = 50\n",
    "\n",
    "raw_corpus = []\n",
    "with open(dataset_file_path, \"r\") as dataset_file:\n",
    "    reader = csv.reader(dataset_file)\n",
    "    \n",
    "    row_number = 0\n",
    "    for row in reader:\n",
    "        raw_corpus.append([row[2], row[4], row[5]])\n",
    "        \n",
    "        if row_number >= n_docs:\n",
    "            break\n",
    "            \n",
    "        row_number += 1\n",
    "\n",
    "# title=2, description=4, text=5 \n",
    "print(raw_corpus[0])\n",
    "\n",
    "corpus = pd.DataFrame(raw_corpus, columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Words Removal and Lemmatization (spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['title', 'description', 'text']\n",
      "title                [title]\n",
      "description    [description]\n",
      "text                  [text]\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "spacy_nlp_pipeline = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "spacy_corpus = []\n",
    "for column in columns:\n",
    "    idx = 0\n",
    "    for document in spacy_nlp_pipeline.pipe(corpus[column]):\n",
    "        if idx >= len(spacy_corpus) or len(spacy_corpus) == 0:\n",
    "            spacy_corpus.append([])\n",
    "        \n",
    "        text = \" \".join([word.lemma_ for word in document if not word.is_stop and not word.is_digit])\n",
    "        \n",
    "        spacy_corpus[idx].append(simple_preprocess(text, deacc=True))\n",
    "        \n",
    "        idx += 1\n",
    "    \n",
    "spacy_corpus = pd.DataFrame(spacy_corpus, columns=columns)\n",
    "print(raw_corpus[0])\n",
    "print(spacy_corpus.loc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Words Removal and Lemmatization (stanza)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-22 21:07:29 INFO: Loading these models for language: en (English):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | ewt     |\n",
      "| lemma     | ewt     |\n",
      "=======================\n",
      "\n",
      "2020-11-22 21:07:29 INFO: Use device: cpu\n",
      "2020-11-22 21:07:29 INFO: Loading: tokenize\n",
      "2020-11-22 21:07:29 INFO: Loading: lemma\n",
      "2020-11-22 21:07:29 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['title', 'description', 'text']\n",
      "title                [title]\n",
      "description    [description]\n",
      "text                  [text]\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "stanza_nlp_pipeline = stanza.Pipeline(\"en\", processors=\"tokenize,lemma\")\n",
    "stopwords = stopwords.words(\"english\")\n",
    "\n",
    "stanza_corpus = []\n",
    "for column in columns:\n",
    "    idx = 0\n",
    "    for document in corpus[column]:\n",
    "        if idx >= len(stanza_corpus) or len(stanza_corpus) == 0:\n",
    "            stanza_corpus.append([])\n",
    "        \n",
    "        preprocessed_text = \" \".join(simple_preprocess(document, min_len=3))\n",
    "        \n",
    "        tokens = []\n",
    "        for sentence in stanza_nlp_pipeline(preprocessed_text).sentences:\n",
    "            for word in sentence.words:\n",
    "                if word.lemma not in stopwords and word.text not in stopwords:\n",
    "                    tokens.append(word.lemma)\n",
    "        \n",
    "        stanza_corpus[idx].append(tokens)\n",
    "        \n",
    "        idx += 1\n",
    "    \n",
    "stanza_corpus = pd.DataFrame(stanza_corpus, columns=columns)\n",
    "print(raw_corpus[0])\n",
    "print(stanza_corpus.loc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sea turtle baby boom hatches amid coronavirus lockdown\n",
      "['sea', 'turtle', 'baby', 'boom', 'hatch', 'amid', 'coronavirus', 'lockdown']\n",
      "['sea', 'turtle', 'baby', 'boom', 'hatches', 'amid', 'coronavirus', 'lockdow']\n",
      "\n",
      "Wuhan health officials raise death toll linked to COVID-19 by 50%\n",
      "['wuhan', 'health', 'official', 'raise', 'death', 'toll', 'link', 'covid']\n",
      "['wuhan', 'health', 'official', 'raise', 'death', 'toll', 'link', 'covid']\n",
      "\n",
      "Did the WHO mishandle the global coronavirus pandemic?\n",
      "['mishandle', 'global', 'coronavirus', 'pandemic']\n",
      "['mishandle', 'global', 'coronavirus', 'pandemic']\n",
      "\n",
      "The latest on the coronavirus outbreak for April 27\n",
      "['late', 'coronavirus', 'outbreak', 'april']\n",
      "['latest', 'coronavirus', 'outbreak', 'april']\n",
      "\n",
      "China angered by Australian call for international inquiry into coronavirus origin\n",
      "['china', 'anger', 'australian', 'international', 'inquiry', 'coronavirus', 'origin']\n",
      "['china', 'angered', 'australian', 'call', 'international', 'inquiry', 'coronavirus', 'origin']\n",
      "\n",
      "Queen City Patrol cleans up downtown as some restrictions set to lift in Sask.\n",
      "['queen', 'city', 'patrol', 'clean', 'downtown', 'restriction', 'set', 'lift', 'sask']\n",
      "['queen', 'city', 'patrol', 'clean', 'downtown', 'restriction', 'set', 'lift', 'sask']\n",
      "\n",
      "Coronavirus will trigger biggest ever plunge in energy demand, emissions, says IEA\n",
      "['coronavirus', 'trigger', 'big', 'plunge', 'energy', 'demand', 'emission', 'say', 'iea']\n",
      "['coronavirus', 'trigger', 'biggest', 'ever', 'plunge', 'energy', 'demand', 'emission', 'say', 'iea']\n",
      "\n",
      "'Very quiet' summer on the horizon for event planners as COVID-19 restrictions continue\n",
      "['quiet', 'summer', 'horizon', 'event', 'planner', 'covid', 'restriction', 'continue']\n",
      "['quiet', 'summer', 'horizon', 'event', 'planner', 'covid', 'restriction', 'continue']\n",
      "\n",
      "Wuhan health officials raise death toll linked to COVID-19 by 50%\n",
      "['wuhan', 'health', 'official', 'raise', 'death', 'toll', 'link', 'covid']\n",
      "['wuhan', 'health', 'official', 'raise', 'death', 'toll', 'link', 'covid']\n",
      "\n",
      "Coronavirus: What's happening in Canada and around the world April 27\n",
      "['coronavirus', 'happen', 'canada', 'world', 'april']\n",
      "['coronavirus', 'happen', 'canada', 'around', 'world', 'april']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# print 3 random titles\n",
    "for _ in range(10):\n",
    "    idx = random.randint(0, 50)\n",
    "    print(raw_corpus[idx][0])\n",
    "    print(spacy_corpus.loc[idx,\"title\"])\n",
    "    print(stanza_corpus.loc[idx,\"title\"])\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
